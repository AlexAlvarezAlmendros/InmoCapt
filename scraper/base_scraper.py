import time
import random
import requests
import logging
from abc import ABC, abstractmethod
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from utils.antibot import antibot_manager

try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    st = None
    STREAMLIT_AVAILABLE = False


class BaseScraper(ABC):
    """Clase base abstracta para todos los scrapers de portales inmobiliarios"""
    
    def __init__(self, name: str, delay: float = 2.0):
        self.name = name
        self.delay = delay
        self.session = requests.Session()
        self.logger = logging.getLogger(f'{__name__}.{self.name}')
        
        # Headers comunes para simular navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(self.headers)
    
    def _update_current_page(self, page: int):
        """Actualizar la p√°gina actual en el session state"""
        if STREAMLIT_AVAILABLE and st and hasattr(st.session_state, 'current_page'):
            st.session_state.current_page = page
            # Tambi√©n a√±adir log de progreso de p√°gina
            if hasattr(st.session_state, 'log_messages'):
                st.session_state.log_messages.append(f"üìÑ {self.name} - Procesando p√°gina {page}")
    
    def _add_log_message(self, message: str):
        """A√±adir mensaje al log si Streamlit est√° disponible"""
        if STREAMLIT_AVAILABLE and st and hasattr(st.session_state, 'log_messages'):
            st.session_state.log_messages.append(message)
    
    def _make_request(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """Realizar petici√≥n HTTP con t√©cnicas anti-bot avanzadas"""
        self.logger.info(f"Realizando petici√≥n con anti-bot a: {url}")
        
        # Usar el sistema anti-bot
        response = antibot_manager.make_request(url)
        
        if response and response.status_code == 200:
            self.logger.debug(f"‚úÖ Petici√≥n exitosa: {response.status_code}")
            return BeautifulSoup(response.content, 'html.parser')
        elif response:
            self.logger.warning(f"‚ùå Respuesta con c√≥digo: {response.status_code}")
            
            # Si es 403, intentar t√©cnicas adicionales
            if response.status_code == 403:
                self.logger.info("üõ°Ô∏è Detectado bloqueo 403, aplicando t√©cnicas avanzadas...")
                return self._handle_403_block(url)
        else:
            self.logger.error(f"‚ùå No se pudo obtener respuesta de: {url}")
            
        return None
    
    def _handle_403_block(self, url: str) -> Optional[BeautifulSoup]:
        """Manejar espec√≠ficamente bloqueos 403 con t√©cnicas adicionales"""
        
        # T√©cnica 1: Delay m√°s largo y nuevo User-Agent
        self.logger.info("T√©cnica 1: Delay extendido + nuevo User-Agent")
        time.sleep(random.uniform(5, 10))
        
        response = antibot_manager.make_request(url, headers={
            'User-Agent': antibot_manager.get_random_user_agent(),
            'Referer': 'https://www.google.com/',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        if response and response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
        
        # T√©cnica 2: Simular navegaci√≥n desde la p√°gina principal
        self.logger.info("T√©cnica 2: Navegaci√≥n desde p√°gina principal")
        base_domain = '/'.join(url.split('/')[:3])
        
        # Visitar primero la p√°gina principal
        antibot_manager.make_request(base_domain)
        time.sleep(random.uniform(2, 5))
        
        # Luego intentar la URL objetivo con referer
        response = antibot_manager.make_request(url, headers={
            'Referer': base_domain
        })
        
        if response and response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
        
        # T√©cnica 3: SELENIUM FALLBACK para casos extremos
        self.logger.info("ü§ñ T√©cnica 3: Selenium Stealth Fallback")
        return self._try_selenium_fallback(url)
    
    def _try_selenium_fallback(self, url: str) -> Optional[BeautifulSoup]:
        """Usar Selenium como √∫ltimo recurso para casos extremos"""
        try:
            from utils.selenium_stealth import selenium_stealth
            
            self.logger.info("üöÄ Iniciando Selenium Stealth para evadir protecci√≥n avanzada...")
            
            # Usar navegaci√≥n humana con Selenium
            soup = selenium_stealth.human_navigation(url, wait_time=(5, 10))
            
            if soup:
                self.logger.info("‚úÖ Selenium Stealth exitoso - p√°gina obtenida")
                return soup
            else:
                self.logger.error("‚ùå Selenium Stealth fall√≥")
                return None
                
        except ImportError:
            self.logger.warning("‚ö†Ô∏è Selenium no disponible. Instala con: pip install selenium")
            self.logger.info("üí° Tambi√©n necesitas ChromeDriver: https://chromedriver.chromium.org/")
            return None
        except Exception as e:
            self.logger.error(f"‚ùå Error con Selenium: {str(e)}")
            return None
    
    def _extract_text(self, element, selector: str, default: str = "") -> str:
        """Extraer texto de un elemento usando selector CSS"""
        try:
            found = element.select_one(selector)
            return found.get_text(strip=True) if found else default
        except Exception as e:
            self.logger.debug(f"Error extrayendo texto con selector {selector}: {e}")
            return default
    
    def _extract_price(self, price_text: str) -> Optional[float]:
        """Extraer precio num√©rico del texto"""
        try:
            # Limpiar texto y extraer n√∫mero
            import re
            price_clean = re.sub(r'[^\d,.]', '', price_text)
            price_clean = price_clean.replace(',', '.')
            return float(price_clean) if price_clean else None
        except ValueError:
            return None
    
    def _is_particular(self, soup: BeautifulSoup) -> bool:
        """Verificar si el anuncio es de un particular"""
        return self._check_particular_indicators(soup)
    
    @abstractmethod
    def _check_particular_indicators(self, soup: BeautifulSoup) -> bool:
        """M√©todo abstracto para verificar indicadores espec√≠ficos de cada portal"""
        pass
    
    @abstractmethod
    def _extract_listing_data(self, soup: BeautifulSoup) -> Dict:
        """M√©todo abstracto para extraer datos espec√≠ficos de cada portal"""
        pass
    
    @abstractmethod
    def build_search_url(self, search_params: Dict) -> str:
        """M√©todo abstracto para construir URL de b√∫squeda espec√≠fica"""
        pass
    
    def search_listings_realtime(self, search_params: Dict) -> List[Dict]:
        """Buscar listados con actualizaciones en tiempo real para la interfaz web"""
        try:
            # Intentar importar streamlit para actualizaciones en tiempo real
            import streamlit as st
            realtime_updates = True
        except (ImportError, RuntimeError):
            # Si no est√° disponible, usar m√©todo normal
            return self.search_listings(search_params)
        
        results = []
        page = 1
        max_pages = search_params.get('max_pages', 10)
        total_processed = 0
        total_particulares = 0
        
        self.logger.info(f"Iniciando b√∫squeda en tiempo real en {self.name} - M√°ximo {max_pages} p√°ginas")
        
        while page <= max_pages:
            # Verificar si se solicit√≥ parar la b√∫squeda
            if self._should_stop_search():
                self.logger.info(f"üõë B√∫squeda interrumpida por el usuario en {self.name} (p√°gina {page})")
                break
            
            # Actualizar p√°gina actual en tiempo real
            if realtime_updates:
                st.session_state.current_page = page
                st.session_state.log_messages.append(f"üìÑ Procesando p√°gina {page} de {self.name}")
                
            self.logger.info(f"Procesando p√°gina {page} de {self.name}")
            
            # Construir URL de la p√°gina actual
            url = self.build_search_url({**search_params, 'page': page})
            soup = self._make_request(url)
            
            if not soup:
                self.logger.error(f"No se pudo obtener la p√°gina {page}")
                if realtime_updates:
                    st.session_state.log_messages.append(f"‚ùå Error cargando p√°gina {page}")
                break
            
            # Extraer enlaces de listados de esta p√°gina
            listings = self._extract_listing_links(soup)
            
            if not listings:
                self.logger.info(f"No se encontraron m√°s listados en p√°gina {page}")
                if realtime_updates:
                    st.session_state.log_messages.append(f"üèÅ Fin de resultados en p√°gina {page}")
                break
            
            self.logger.info(f"Encontrados {len(listings)} listados en p√°gina {page}")
            if realtime_updates:
                st.session_state.log_messages.append(f"üîç Encontrados {len(listings)} anuncios en p√°gina {page}")
            
            # Procesar cada listado individual
            page_particulares = 0
            for i, listing_url in enumerate(listings, 1):
                # Verificar interrupci√≥n antes de procesar cada listado
                if self._should_stop_search():
                    self.logger.info(f"üõë B√∫squeda interrumpida durante procesamiento de listado {i} en p√°gina {page}")
                    return results  # Retornar resultados obtenidos hasta ahora
                
                progress_msg = f"Procesando listado {i}/{len(listings)} de p√°gina {page}"
                self.logger.debug(f"{progress_msg}: {listing_url}")
                
                listing_data = self.scrape_listing(listing_url)
                if listing_data:
                    results.append(listing_data)
                    page_particulares += 1
                    total_particulares += 1
                    
                    # Actualizaci√≥n en tiempo real
                    if realtime_updates:
                        st.session_state.listings_found = total_particulares
                        st.session_state.log_messages.append(f"‚úÖ Particular #{total_particulares}: {listing_data.get('titulo', 'Sin t√≠tulo')[:50]}...")
                    
                    self.logger.info(f"‚úÖ Particular encontrado ({total_particulares} total): {listing_data.get('titulo', 'Sin t√≠tulo')}")
                else:
                    self.logger.debug(f"‚ùå Descartado (no particular): {listing_url}")
                
                total_processed += 1
            
            self.logger.info(f"P√°gina {page} completada: {page_particulares} particulares de {len(listings)} listados")
            if realtime_updates:
                st.session_state.log_messages.append(f"üìä P√°gina {page}: {page_particulares} particulares de {len(listings)} anuncios")
            
            page += 1
        
        if not self._should_stop_search():
            self.logger.info(f"B√∫squeda completada en {self.name}: {total_particulares} particulares de {total_processed} listados procesados")
            if realtime_updates:
                st.session_state.log_messages.append(f"üéØ {self.name} completado: {total_particulares} particulares encontrados")
        else:
            self.logger.info(f"B√∫squeda interrumpida en {self.name}: {total_particulares} particulares de {total_processed} listados procesados hasta la interrupci√≥n")
            if realtime_updates:
                st.session_state.log_messages.append(f"üõë {self.name} interrumpido: {total_particulares} particulares guardados")
        
        return results
    
    def search_listings(self, search_params: Dict) -> List[Dict]:
        """Buscar listados bas√°ndose en par√°metros de b√∫squeda con capacidad de interrupci√≥n"""
        results = []
        page = 1
        max_pages = search_params.get('max_pages', 10)
        total_processed = 0
        total_particulares = 0
        
        self.logger.info(f"Iniciando b√∫squeda en {self.name} - M√°ximo {max_pages} p√°ginas")
        
        while page <= max_pages:
            # Verificar si se solicit√≥ parar la b√∫squeda (desde Streamlit session_state)
            if self._should_stop_search():
                self.logger.info(f"üõë B√∫squeda interrumpida por el usuario en {self.name} (p√°gina {page})")
                break
            
            # Actualizar p√°gina actual en session state
            self._update_current_page(page)
                
            self.logger.info(f"Procesando p√°gina {page} de {self.name}")
            
            # Construir URL de la p√°gina actual
            url = self.build_search_url({**search_params, 'page': page})
            soup = self._make_request(url)
            
            if not soup:
                self.logger.error(f"No se pudo obtener la p√°gina {page}")
                break
            
            # Extraer enlaces de listados de esta p√°gina
            listings = self._extract_listing_links(soup)
            
            if not listings:
                self.logger.info(f"No se encontraron m√°s listados en p√°gina {page}")
                break
            
            self.logger.info(f"Encontrados {len(listings)} listados en p√°gina {page}")
            
            # Procesar cada listado individual
            page_particulares = 0
            for i, listing_url in enumerate(listings, 1):
                # Verificar interrupci√≥n antes de procesar cada listado
                if self._should_stop_search():
                    self.logger.info(f"üõë B√∫squeda interrumpida durante procesamiento de listado {i} en p√°gina {page}")
                    return results  # Retornar resultados obtenidos hasta ahora
                
                progress_msg = f"Procesando listado {i}/{len(listings)} de p√°gina {page}"
                self.logger.debug(f"{progress_msg}: {listing_url}")
                
                # A√±adir log cada 5 listados procesados para no saturar
                if i % 5 == 0:
                    self._add_log_message(f"üìã {self.name} - Procesando {i}/{len(listings)} listados de p√°gina {page}")
                
                listing_data = self.scrape_listing(listing_url)
                if listing_data:
                    results.append(listing_data)
                    page_particulares += 1
                    total_particulares += 1
                    self.logger.info(f"‚úÖ Particular encontrado ({total_particulares} total): {listing_data.get('titulo', 'Sin t√≠tulo')}")
                    
                    # A√±adir log al session state cuando se encuentre un particular
                    self._add_log_message(f"üè† {self.name} - Particular #{total_particulares}: {listing_data.get('titulo', 'Sin t√≠tulo')[:50]}...")
                else:
                    self.logger.debug(f"‚ùå Descartado (no particular): {listing_url}")
                
                total_processed += 1
            
            self.logger.info(f"P√°gina {page} completada: {page_particulares} particulares de {len(listings)} listados")
            page += 1
        
        if not self._should_stop_search():
            self.logger.info(f"B√∫squeda completada en {self.name}: {total_particulares} particulares de {total_processed} listados procesados")
        else:
            self.logger.info(f"B√∫squeda interrumpida en {self.name}: {total_particulares} particulares de {total_processed} listados procesados hasta la interrupci√≥n")
        
        return results
    
    def _should_stop_search(self) -> bool:
        """Verificar si se debe parar la b√∫squeda (desde Streamlit session_state)"""
        try:
            # Intentar importar streamlit y verificar session_state
            import streamlit as st
            return st.session_state.get('stop_search', False)
        except (ImportError, RuntimeError, AttributeError):
            # Si no est√° disponible streamlit o session_state, continuar normalmente
            return False
    
    @abstractmethod
    def _extract_listing_links(self, soup: BeautifulSoup) -> List[str]:
        """Extraer enlaces de listados de la p√°gina de resultados"""
        pass
    
    def scrape_listing(self, url: str) -> Optional[Dict]:
        """
        Analizar p√°gina de detalle de vivienda.
        Proceso:
        1. Abrir p√°gina de detalle
        2. Verificar si el anunciante es particular
        3. Si es particular: extraer datos y retornar
        4. Si no es particular: retornar None (no se guarda)
        """
        self.logger.debug(f"Analizando detalle: {url}")
        
        soup = self._make_request(url)
        if not soup:
            self.logger.warning(f"No se pudo cargar la p√°gina: {url}")
            return None
        
        # PASO 1: Verificar si es particular antes de extraer datos
        if not self._is_particular(soup):
            self.logger.debug(f"‚ùå No es particular, saltando: {url}")
            return None
        
        # PASO 2: Si es particular, extraer todos los datos
        self.logger.debug(f"‚úÖ Es particular, extrayendo datos: {url}")
        
        try:
            data = self._extract_listing_data(soup)
            data['url'] = url
            data['portal'] = self.name
            
            # Validar que se extrajeron datos m√≠nimos
            if not data.get('titulo') and not data.get('precio'):
                self.logger.warning(f"Datos insuficientes extra√≠dos de: {url}")
                return None
            
            return data
            
        except Exception as e:
            self.logger.error(f"Error extrayendo datos de {url}: {str(e)}")
            return None
        
        return data
