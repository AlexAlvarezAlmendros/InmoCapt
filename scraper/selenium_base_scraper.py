#!/usr/bin/env python3
"""
Scraper base especializado en Selenium para portales con protecci√≥n anti-bot fuerte
Optimizado para m√°ximo rendimiento y evasi√≥n
"""

import time
import random
import logging
import re
from abc import ABC, abstractmethod
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from utils.selenium_stealth import selenium_stealth

# Configurar logging silencioso para librer√≠as de Selenium
logging.getLogger('selenium').setLevel(logging.CRITICAL)
logging.getLogger('urllib3').setLevel(logging.CRITICAL)
logging.getLogger('webdriver_manager').setLevel(logging.CRITICAL)

class SeleniumBaseScraper(ABC):
    """Clase base para scrapers que usan Selenium como m√©todo principal"""
    
    def __init__(self, name: str, delay: float = 5.0, headless: bool = False):  # Cambiar a False para mejor evasi√≥n DataDome
        self.name = name
        self.delay = delay
        self.headless = headless
        self.logger = logging.getLogger(f"selenium.{name}")
        self.session_pages = 0  # Contador de p√°ginas por sesi√≥n
        self.max_session_pages = 999999  # Desactivar reinicio autom√°tico - mantener sesi√≥n activa
        
    def _make_request(self, url: str) -> Optional[BeautifulSoup]:
        """Realizar petici√≥n usando Selenium como m√©todo principal"""
        # Logging m√°s silencioso - solo para URLs importantes
        if "venta-viviendas" in url and "pagina" not in url:
            self.logger.info(f"Selenium: {url}")
        
        # Gesti√≥n de sesi√≥n - NO reiniciar autom√°ticamente
        # Comentado para mantener sesi√≥n activa sin interrupciones
        # if self.session_pages >= self.max_session_pages:
        #     self.logger.info("üîÑ Reiniciando sesi√≥n Selenium para evitar detecci√≥n")
        #     selenium_stealth.close()
        #     self.session_pages = 0
        
        # Configurar driver si es necesario
        if not selenium_stealth.driver:
            if not selenium_stealth.setup_driver(headless=self.headless):
                self.logger.error("ERROR: No se pudo configurar Selenium WebDriver")
                return self._fallback_http_request(url)
        
        try:
            # Usar navegaci√≥n humana optimizada
            soup = selenium_stealth.human_navigation(url, wait_time=(3, 6))
            
            if soup and self._validate_selenium_content(soup, url):
                self.session_pages += 1
                # Solo log cada 10 p√°ginas para reducir ruido
                if self.session_pages % 10 == 0:
                    self.logger.info(f"Progreso: {self.session_pages} paginas procesadas")
                return soup
            else:
                self.logger.warning("AVISO: Contenido Selenium invalido, intentando HTTP fallback")
                return self._fallback_http_request(url)
                
        except Exception as e:
            self.logger.error(f"ERROR: Error Selenium: {str(e)}")
            return self._fallback_http_request(url)
    
    def _validate_selenium_content(self, soup: BeautifulSoup, url: str) -> bool:
        """Validar que el contenido obtenido por Selenium es v√°lido"""
        
        # Validaci√≥n b√°sica
        if not soup or len(str(soup)) < 1000:
            return False
        
        # Verificar t√≠tulo
        title = soup.find('title')
        if not title:
            return False
        
        title_text = title.get_text().lower()
        
        # Verificar que no es p√°gina de error
        error_indicators = ['error', 'blocked', 'access denied', 'forbidden']
        if any(indicator in title_text for indicator in error_indicators):
            return False
        
        # Validaci√≥n espec√≠fica por portal
        return self._validate_portal_content(soup, url)
    
    @abstractmethod
    def _validate_portal_content(self, soup: BeautifulSoup, url: str) -> bool:
        """Validaci√≥n espec√≠fica del portal - implementar en cada scraper"""
        pass
    
    def _fallback_http_request(self, url: str) -> Optional[BeautifulSoup]:
        """Fallback a HTTP tradicional si Selenium falla"""
        try:
            from utils.antibot import antibot_manager
            
            self.logger.info("üîÑ Fallback a HTTP tradicional")
            response = antibot_manager.make_request(url)
            
            if response and response.status_code == 200:
                return BeautifulSoup(response.content, 'html.parser')
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå Fallback HTTP tambi√©n fall√≥: {str(e)}")
            return None
    
    def search_listings(self, search_params: Dict) -> List[Dict]:
        """Buscar listados usando Selenium como m√©todo principal"""
        results = []
        page = 1
        max_pages = 999  # Revisar todas las p√°ginas disponibles
        total_processed = 0
        total_particulares = 0
        
        self.logger.info(f"INICIO: Iniciando busqueda Selenium en {self.name} - Revisando todas las paginas")
        
        while page <= max_pages:
            # Solo mostrar progreso cada 5 p√°ginas
            if page % 5 == 1 or page <= 3:
                self.logger.info(f"Procesando pagina {page} de {self.name}")
            
            # Construir URL de la p√°gina actual
            url = self.build_search_url({**search_params, 'page': page})
            soup = self._make_request(url)
            
            if not soup:
                self.logger.error(f"‚ùå No se pudo obtener la p√°gina {page}")
                break
            
            # Extraer enlaces de listados de esta p√°gina
            listings = self._extract_listing_links(soup)
            
            if not listings:
                self.logger.info(f"üèÅ No se encontraron m√°s listados en p√°gina {page}")
                break
            
            self.logger.info(f"üîç Encontrados {len(listings)} listados en p√°gina {page}")
            
            # Procesar cada listado individual
            page_particulares = 0
            for i, listing_url in enumerate(listings, 1):
                progress_msg = f"Procesando listado {i}/{len(listings)} de p√°gina {page}"
                self.logger.debug(f"{progress_msg}: {listing_url}")
                
                listing_data = self.scrape_listing(listing_url)
                if listing_data:
                    results.append(listing_data)
                    page_particulares += 1
                    total_particulares += 1
                    self.logger.info(f"‚úÖ Particular encontrado ({total_particulares} total): {listing_data.get('titulo', 'Sin t√≠tulo')}")
                else:
                    self.logger.debug(f"‚ùå Descartado (no particular): {listing_url}")
                
                total_processed += 1
                
                # Delay entre listados para evitar detecci√≥n
                time.sleep(random.uniform(1, 3))
            
            self.logger.info(f"üìä P√°gina {page} completada: {page_particulares} particulares de {len(listings)} listados")
            page += 1
            
            # Delay entre p√°ginas
            time.sleep(random.uniform(3, 6))
        
        self.logger.info(f"üéØ B√∫squeda completada en {self.name}: {total_particulares} particulares de {total_processed} listados procesados")
        return results
    
    def scrape_listing(self, url: str) -> Optional[Dict]:
        """Scraper listado individual usando Selenium"""
        self.logger.debug(f"üîç Analizando detalle con Selenium: {url}")
        
        soup = self._make_request(url)
        if not soup:
            self.logger.warning(f"‚ö†Ô∏è No se pudo cargar la p√°gina: {url}")
            return None
        
        # Verificar si es particular antes de extraer datos
        if not self._is_particular(soup):
            self.logger.debug(f"‚ùå No es particular, saltando: {url}")
            return None
        
        # Si es particular, extraer todos los datos
        self.logger.debug(f"‚úÖ Es particular, extrayendo datos: {url}")
        
        try:
            data = self._extract_listing_data(soup)
            data['url'] = url
            data['portal'] = self.name
            data['method'] = 'selenium'  # Marcar como obtenido por Selenium
            
            # Validar que se extrajeron datos m√≠nimos
            if not data.get('titulo') and not data.get('precio'):
                self.logger.warning(f"‚ö†Ô∏è Datos insuficientes extra√≠dos de: {url}")
                return None
            
            return data
            
        except Exception as e:
            self.logger.error(f"‚ùå Error extrayendo datos de {url}: {str(e)}")
            return None
    
    @abstractmethod
    def build_search_url(self, params: Dict) -> str:
        """Construir URL de b√∫squeda - implementar en cada scraper"""
        pass
    
    @abstractmethod
    def _extract_listing_links(self, soup: BeautifulSoup) -> List[str]:
        """Extraer enlaces de listados - implementar en cada scraper"""
        pass
    
    @abstractmethod
    def _is_particular(self, soup: BeautifulSoup) -> bool:
        """Verificar si es particular - implementar en cada scraper"""
        pass
    
    @abstractmethod
    def _extract_listing_data(self, soup: BeautifulSoup) -> Dict:
        """Extraer datos del listado - implementar en cada scraper"""
        pass
    
    def close_session(self):
        """Cerrar sesi√≥n Selenium"""
        selenium_stealth.close()
        self.session_pages = 0
        self.logger.info("üîí Sesi√≥n Selenium cerrada")
    
    def __del__(self):
        """Destructor - cerrar sesi√≥n autom√°ticamente"""
        try:
            self.close_session()
        except:
            pass
    
    # M√©todos helper para compatibilidad con scrapers existentes
    def _extract_text(self, element, selector: str, default: str = "") -> str:
        """Extraer texto de un elemento usando selector CSS"""
        try:
            found = element.select_one(selector)
            return found.get_text(strip=True) if found else default
        except Exception:
            return default
    
    def _extract_price(self, price_text: str) -> int:
        """Extraer precio num√©rico de texto"""
        try:
            # Eliminar todo excepto n√∫meros
            numbers = re.findall(r'\d+', price_text.replace('.', '').replace(',', ''))
            if numbers:
                return int(''.join(numbers))
            return 0
        except Exception:
            return 0
    
    def _extract_number(self, text: str) -> int:
        """Extraer n√∫mero de texto"""
        try:
            numbers = re.findall(r'\d+', text)
            return int(numbers[0]) if numbers else 0
        except Exception:
            return 0
